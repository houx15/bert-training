import torch

import numpy as np

from transformers import AutoTokenizer, AutoModelForSequenceClassification
from utils import sentence_cleaner


class OpinionPredict(object):
    r"""
    Make predictions based on trained models.

    Args:
        task_type (`str`):
            Current task type, 'regression' or 'binary'.
        model_path (`str`):
            Model name or path, e.g. '../model-bert/gun-regression'. The model will be used for tokenization and prediction.
        src_type (`str`, *optional*, defaults to `tweet`):
            Data source. 'tweet' or 'weibo
    """

    def __init__(
        self, task_type: str, model_path: str, src_type: str = "weibo"
    ) -> None:
        assert task_type in ["regression", "binary"]
        self.task_type = task_type
        self.src_type = src_type

        self.tokenizer = AutoTokenizer.from_pretrained(model_path, do_lower_case=False)
        self.model = self.model_init(model_path)

    def model_init(self, model_path: str):
        print(">>>>>>>>>initializing model:  {}".format(model_path))

        num_labels_map = {
            "binary": 2,
            "regression": 1,
        }
        torch.cuda.empty_cache()
        model = AutoModelForSequenceClassification.from_pretrained(
            model_path,
            num_labels=num_labels_map[self.task_type],
            output_attentions=True,
            output_hidden_states=False,
        )
        if torch.cuda.is_available():
            model.cuda()
        self.model = model
        return self.model

    def predict(
        self,
        texts: list,
        max_length: int = None,
        padding: str = "max_length",
        batch: int = 512,
        verbose=print,
    ) -> np.array:
        """
        :param texts: list (or numpy.array, pandas.Series, torch.tensor) of strings.
        :param padding: str. 'longest', 'max_length' (default), 'do_not_pad'.
        :param max_length, batch: int. max length of tweet and number of tweets proceeded in a batch. limited by GPU memory.
            max_length=128 is sufficient for most tweets, and 512 tweeets per batch are recommended for 128-letter tweets on a typical Tesla GPU with 16GB memory.
            :param verbose: function that accepts string input. used to display message.
            e.g., verbose = print (default, display on screen)
            e.g., verbose = logger.info (write to a log file)
            e.g., verbose = lambda message: logger.info(f'filename: {message}') # write to a log file with a header
        :return: 1-dim numpy array
        :example:
            >>> from predict import OpinionPredict
            >>> OpinionPredict(task_type='regression', src_type='tweet', model_path='../model-bert/2022-06-24-yuxin-gun-regression'
            ).predict(texts=[
                "RT After the shooting, Jim and his wife Sarah dedicated their lives to preventing gun violence. They were lifelong Republicans and gun owners themselves. They realized that passing sensible gun laws isn't about politics; it's about saving lives. #GunReform ",
                'Did u hear the gunshot in the video, when someone is rushing you and u hear a gunshot, they could have a gun so u shoot them. Someone else fired a gun.',
                "It didn't though. You can literally 3d print a gun now anyways, no use in banning them. Also that's a one way ticket to all out civil war",
                'I repeat, the 2nd amendment is not on trial here. Kyle did not engage, others engaged him. You are allowed to eliminate as many threats as is necessary to preserve your own life. There is no limit after which the right to your own life becomes inferior to that of your attackers.'
            ])

            output: array([ 1.7457896 ,  1.0045699 ,  0.07550862, -0.76812345], dtype=float32)
            ground truth: [2, 1, 0, -1]

            >>> OpinionPredict(task_type='climate', src_type='weibo', model_path='../../model-bert/2022-11-04-yuxin-climate'
            ).predict(texts=[
                    # label 1
                    "#å…¨çƒå˜æš–##å½“å‰å…¨çƒå˜æš–ç¨‹åº¦ä¸¤åƒå¹´æœªé‡#  http://f.us.sinaimg.cn/003buVDvlx07vIPXCxrq010412006FkO0E010.mp4?label=mp4_720p&template=720x1280.24.0&trans_finger=37e3fed30081d60f956dbe10b6ff7523&Expires=1564191000&ssig=CffYR9pMXq&KID=unistore,video",
                    "æ¾³æ´²çƒ­çš„è¦æ­»ï¼Œç¾å›½ä¸œéƒ¨å†»å¾—è¦æ­»ã€‚ä¸­å›½å¿½å†·å¿½çƒ­â€¦â€¦æç«¯æ°”å€™è¶Šæ¥è¶Šå¤š#å…¨çƒæ°”å€™å˜åŒ–#  See extreme weather across the globe",
                    "å¤©ç†ä¸ä½†è§„å®šç€ä¸ªäººçš„å› æœæŠ¥åº”ï¼Œä¹Ÿè§„å®šäº†ä¸€ä¸ªåœ°åŒºï¼Œä¸€ä¸ªæœä»£ï¼Œä¸€ä¸ªå›½å®¶ç”šè‡³ä¸€ä¸ªä¸–ç•Œçš„å› æœæŠ¥åº”ã€‚å¤©ç¾äººç¥¸å°±æ˜¯å…±ä¸šæ‰€é€ çš„æ¶ä¹‹æœæŠ¥ï¼Œæ¸©å®¤æ•ˆåº”å°±æ˜¯ä¸€ä¸ªæœ€å¥½çš„ä¾‹å­ã€‚ä¸€åˆ‡éƒ½åœ¨å¤©ç†çš„èŒƒç•´ä¹‹å†…ã€‚ ã€è°åœ¨æ“çºµå‘½è¿ã€‘ ",
                    # label 2
                    "åœ°é“å¹¿æ’­å¼€å§‹é€åœ£è¯ç¥ç¦äº† å°æ¸©é¦¨[å¿ƒ][å¿ƒ][å¿ƒ]ã€‚ é—¨å£ä¸ºå…¨çƒå˜æš–è®ºå›å‡†å¤‡çš„å†°å› ä¸ºæœ€è¿‘é™æ¸©è¶Šç»“è¶Šå¤§[ç¬‘cry] é¡ºä¾¿åæ§½ä¸‹#å…¨çƒå˜æš–# æ˜Ÿçƒè¿›åŒ–å¤šä¹ˆå®è§‚çš„è¯¾é¢˜ äººç±»ä¸è¿‡å¯„å±…çš„ğŸœ æ˜¯ä¸æ˜¯å¤ªæŠŠè‡ªå·±å½“å›äº‹äº† å¦‚æœäººç±»ä¸å¹¸çœŸçš„æ¯äº†æ˜Ÿçƒ ä¹Ÿä¸è¿‡æ˜¯è¿™ä¸ªæ˜Ÿçƒçš„å®¿å‘½è€Œå·²[åƒç“œ]#ä»Šæ—¥è´´çº¸æ‰“å¡#  ",
                    "å‘è¡¨äº†ä¸€ç¯‡è½¬è½½åšæ–‡ã€Š[è½¬è½½]ã€æ–¯è¯ºç™»ã€‘â€œå…¨çƒå˜æš–æ˜¯ä¸€ä¸ªç”±ä¸­æƒ…å±€å‘æ˜çš„éª—å±€â€ã€‹[è½¬è½½]ã€æ–¯è¯ºç™»ã€‘â€œ...",
                    "ã€Šå…¨çƒå˜æš–å·²ä¸å¤å­˜åœ¨ï¼Œåœ°çƒæ­£å¤„äºå˜å†·çŠ¶æ€ï¼Œç§‘å­¦å®¶è®¤ä¸ºæ­¤äº‹ä¸ç®€å•ã€‹å…¨çƒå˜æš–å·²ä¸å¤å­˜åœ¨ï¼Œåœ°çƒæ­£å¤„äºå˜å†·çŠ¶æ€ï¼Œç§‘å­¦å®¶è®¤ä¸ºæ­¤äº‹ä¸ç®€å•",
                    # label 3
                    "åˆæœ‰è°èƒ½æƒ³åˆ° æˆ‘åœ¨ä¸‰åå‡ åº¦çš„é«˜æ¸©ä¸‹è¿˜åœ¨çŠ¯é£æ¹¿ è¿˜è¦ä¹°è†è¯è´´å‘¢ è¿™ä¸ªå¤©ä¸å¼€ç©ºè°ƒè¦çƒ­æ­» å¼€äº†ç©ºè°ƒè…¿è¦ç—›æ­» å¤§å®¶å†¬å¤©ä¸€å®šè¦ç©¿ç§‹è£¤ å¥½å¥½å…»ç”Ÿå§[è·ªäº†] ",
                    "ä»Šå¤©è€å¦ˆè¦å›å»äº†ï¼Œä¼¤æ„Ÿæƒ…ç»ªæˆ›ç„¶è€Œç”Ÿï¼Œå›æƒ³èµ·æ¥è¿™ä¸€ä¸ªæœˆé‡Œï¼Œè‡ªå·±è¿æ‹–æŠŠéƒ½æ²¡æœ‰æ‹¾èµ·è¿‡ï¼ŒçœŸæ˜¯æƒ­æ„§ï¼è€å¦ˆå› èº«ä½“åŸå› è€Œä¸èƒ½å¹ç©ºè°ƒï¼Œæ¯å¤©å¿è€ç‚çƒ­é«˜æ¸©è¿˜ç»™æˆ‘ä»¬åšç€å„ç§å®¶åŠ¡ï¼Œä¹Ÿä¸å–Šç´¯ï¼Œä¸å–Šçƒ­ï¼æ¥åˆ°é™Œç”Ÿçš„ç¯å¢ƒï¼Œæ¯å¤©ä¹Ÿå°±åªèƒ½æ•£æ•£æ­¥çœ‹çœ‹ç”µè§†ææå«ç”Ÿï¼Œæ¶ˆé£æ—¶é—´äº†ã€‚è€å¦ˆåœ¨ï¼Œæ¯å¤©æ‰æœ‰å¯å£çš„é¥­èœï¼Œæ‰ä¸ä¼šæ„Ÿåˆ°å­¤å• ",
                    "åªæœ‰åœ¨åˆšå…¥æ‰‹æ–°åŒ–å¦†å“çš„é‚£å‡ å¤©æ‰ä¼šè®¤çœŸçš„æŠ¹è„¸ï¼Œæ²¡è¿‡ä¸€ä¸ªæœˆå°±å¼€å§‹çæ¶‚äº†ï¼Œè¿˜æœ‰å’Œæˆ‘ä¸€æ ·çš„ä¸‰åˆ†é’Ÿçƒ­åº¦å¥³å­©å—[ç¬‘cry][ç¬‘cry][ç¬‘cry] "
                ])

            output:

                array([[ 2.55624795, -1.37531841, -2.05213404],
                        [ 2.51814318, -1.52832699, -1.71303737],
                        [ 0.04927956,  0.15148288, -0.77340394],
                        [ 2.56860423, -2.00646591, -1.2080487 ],
                        [-0.30993834,  0.95506799, -1.47718608],
                        [ 0.41168666,  0.30903456, -1.81370771],
                        [-1.64383245, -1.7373091 ,  3.69929457],
                        [-1.74833012, -1.41184819,  3.30613136],
                        [-1.17227781, -1.62402332,  3.16496921]])

            ground truth:
                [[1,0,0],
                    [1,0,0],
                    [1,0,0],
                    [0,1,0],
                    [0,1,0],
                    [0,1,0],
                    [0,0,1],
                    [0,0,1],
                    [0,0,1]]
        """

        verbose(
            f"predict(texts={len(texts)}, max_length={max_length}, padding={padding}, batch={batch})"
        )
        self.model.eval()
        for parameter in self.model.parameters():
            parameter.requires_grad = False  # freeze the finetuned model. save memory.
        torch.cuda.empty_cache()
        try:
            prediction = np.array([])
            verbose(f"initial prediction.shape={prediction.shape}")

            for i in range(0, len(texts), batch):
                # encode input texts
                encoding = self.tokenizer(
                    [
                        sentence_cleaner(self.src_type, single_text)
                        for single_text in texts[i : i + batch]
                    ],
                    add_special_tokens=True,
                    return_token_type_ids=True,
                    truncation=True,
                    padding=padding,
                    return_attention_mask=True,
                    return_tensors="pt",
                    max_length=max_length,
                )
                if torch.cuda.is_available():
                    # verbose(f'self.model.device={self.model.device}')  # self.model.device=cuda:0
                    for key in encoding.keys():
                        encoding[key] = encoding[key].cuda()
                        # verbose(f'encoding[{key}].device={encoding[key].device}. encoding[{key}].shape={encoding[key].shape}') # encoding[input_ids].device=cuda:0. encoding[input_ids].shape=torch.Size([4, 128])

                # calculate the encoded input with frozen model
                outputs = self.model(
                    input_ids=encoding["input_ids"],
                    attention_mask=encoding["attention_mask"],
                    token_type_ids=encoding["token_type_ids"],
                ).logits.detach()
                # verbose(f'type(outputs)={type(outputs)}, outputs.device={outputs.device}') #  type(outputs)=<class 'torch.Tensor'>, outputs.device=cuda:0
                if torch.cuda.is_available():
                    outputs = (
                        outputs.cpu()
                    )  # copy the tensor to host memory before converting it to numpy. otherwise we will get an error "can't convert cuda:0 device type tensor to numpy"
                    # verbose(f'type(outputs)={type(outputs)}, outputs.device={outputs.device}')  # type(outputs)=<class 'torch.Tensor'>, outputs.device=cpu
                del encoding
                verbose(f"outputs.numpy().shape={outputs.numpy().shape}")
                # verbose(f'outputs.numpy()={outputs.numpy()}')

                # output
                if "regression" == self.task_type:
                    prediction = np.append(
                        prediction, outputs.numpy().flatten()
                    )  # a score for each input string
                elif "binary" == self.task_type:
                    prediction = np.append(
                        prediction, np.argmax(outputs.numpy(), axis=1)
                    )  # a float probability of label "1" for each input string
                else:
                    raise ValueError(f"Unknown self.task_type = {self.task_type}.")
                del outputs
                torch.cuda.empty_cache()
                verbose(f"prediction.shape={prediction.shape}")
                # verbose(f'prediction={prediction}')

            # output
            return prediction

        except RuntimeError as error:
            verbose(f"Running out of memory, retrying with a smaller batch.")
            raise RuntimeError(
                "Running out of GPU memory. Try limiting [max_length] and [batch]."
            ) from error

    # predict()
